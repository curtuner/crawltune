* 爬虫的基本框架
** 基本元素
  爬虫的基本元素包括：
  - URL管理器，用于进行深度受限搜索
  - 一个http客户端程序，如requests或者aiohttp
  - 页面解析器
** 主体逻辑
   首先往 /URL管理器/ 中输入一个或者多个URL之后，之后不断从 /URL管理器/ 中获取新的URL，利用 /HTTP客户端/ 获取该页面，使用 /页面解析器/ 解析URL在放到URL管理器中。
   一般来说主要还是解决几个基本的问题，之后才是解决直接的问题。
** 软件方案
   | 模块       | 库/算法          |
   |------------+------------------|
   | HTTP客户端 | requests/aiohttp |
   | 页面解析器 | 暂不定           |
   | URL管理器  | 深度受限         |
** 原型1
   URL管理器使用列表，http客户端使用requests，页面解析器先不做处理。
** 原型2（页面解析器，解析URL功能）
   URL管理器的资源来自于页面解析器，所以先从页面解析器开始，先能够添加URL。列表暂时足够。一般来说使用正则表达式就足够了，但是个人不喜欢正则，毕竟几乎记不住，所以使用更加简单的工具。可以使用Beautiful Soup。BeautifulSoup，可以看到对于html的使用比较复杂，基本使用这个库也不一定能得到想要的url。
   所以接下来要过滤出合理的URL，这里参考《集体编程智慧》的做法，同时为了方便，使用一个文本文件作为url管理器先。起名data.txt.
*** beautiful soup
    可以从HTML或XML文件中提取数据的Python库
*** url解析要点（需要使用的使用找一下就可以了，不用记）
    1) 在a标签内。
    2) 是否有href属性。
    3) 是否是完整路径，若不是要弄完整，使用urljoin。
    4) 是否含有段落，有则去掉。
    5) 是否属于某个协议，看情况，一般使用urljoin有能力处理多种协议，所以这里要判断。
    目前还没考虑太多
** 原型3（添加url管理器）
   URL管理器使用set就可以检查是否重复了，方便起见完成一个队列来处理。list用于队列效率不高，所以使用queue库的Queue。主要维护两个数据结构，url队列要保证获取了页面获取
   经过测试，没有出现问题了，主要是处理网页不能获取的问题，这是常见的。目前已经可以完成对于网页的访问和新网页的获取了，之后就是处理网页内容了。
* 任务管理
  对于爬虫来说通过url获取页面是必须的，也是基本的，之后是通过分析器来解析接下来获取的恶页面，所以总体上分成两个集合。决定下一个需要获取的url是必要的。
* 日志
  Python的日志系统由logging提供
* asyncio.coroutine
  可以把一个生成器编程一个协程，yeild主要是为了让我们调用另一个generator。一般来说有yield关键字的函数，调用函数的时候会生成迭代器。
  本质上这个还是协程，协程是一个用户级线程的概念，本质上多个协程是可以共享同一个线程的。yield from 是切换点。
  通过事件来控制协程的运行，某个协程运行了
  在Pyhon中协程是特殊的生成器。
  大家都不明白，异步其实就是并行
  所谓的异步接口，其实就是通过事件来返回结果的io，而不会阻塞，另一个问题。
  事件和事件处理函数的绑定是一个比较麻烦的话题，用协程其实比较好地处理了这个问题。协程的类型，
  await只有在协程中才能起作用.
  需要注意的是yield表达式是传回参数给生成器调用者，而返回值则是给内部的逻辑的。返回值由send来设定，而默认情况下返回None。
  目前已经兼容了python3.5的await和async了。
* 管理URL
* 获取页面
  首先获取页面如果使用的是requests的话，由于requests是同步的，所以容易阻塞在这里。为了防止阻塞的问题，就需要异步获取页面。使用aiohttp来完成。之后还要处理重定向的问题，我们需要设定重定向的深度。首先是网络不稳定，需要有几次重试机会。
  #+BEGIN_SRC python

  #+END_SRC
* 分析页面
* 协程的出现是为了更好的炸干CPU的性能，提升并行性
  其实就是事件循环的另一个形式，默认情况下是可以表示称为这种形式的
* 使用async和await语法的coroutine
  取消yield from 最重要的原因是对于协程和生成器类型不做区分。
* 思考
  - 一个程序包括基本元素，和主体逻辑，之后就是一些特殊的处理。
  - 字典里面有时如果不确定是否会有一个属性，就选择是个get
  - html标签a也可以没有href
  - None是会被print打印的
* 参考
  - https://www.crummy.com/software/BeautifulSoup/bs4/doc/index.zh.html
  
